MEMORY OPTIMIZATION

1. Виконувати зчитування файлу будемо з допомогою BufferedReader, який більш ефективний по оптимізації пам'яті
за рахунок того, що він не зберігає в себе в буфері дані з файлу, а поступово слово за словом зчитує файл, а дані з буферу
зберігаються тимчасово. Files.readAllBytes() зчитує цілий файл за раз, за рахунок цього, якщо файл досить великий, йому буде потрібно
більше пам'яті, оскільки в буфер ми пміщаємо одразу увесь файл. 
2. Замість String.split() для кожного рядка ми використаєм клас Scanner - він опрацьовує слово і ми одразу можемо його помістити в мапу, 
яка вже вміщує дані про кількість повторень. 
3. З одного боку HashMap може потребувати більше пам'яті, але оскільки ми не можемо визначити кількість слів, які є унікальними
і визначити розмір потребуваного масиву, HashMap натомість росте динамічно. Для малих об'ємів даних звичайні масиви - це супер,
але для великих об'ємів даних - краще використовувати спеціальні класи
4. Найосновніше ми прибрали створення нових строк - distinctString, під час додавання нових слів наша початкова строка не 
оновлюється, а просто створюється нова. Якщо в нас буде 1000 унікальних слів - створиться 1000 нових об'єктів, що зовсім 
не оптимізовано.
5. Заради цікавості подивимось скільки памяті ми використовуєм після оптимізації

      ДО - ![image](https://github.com/holsoni/reingeneering-lab2/assets/70883268/23280641-954c-4f2c-89a5-72b07b0325f7)
 
     ПІСЛЯ - ![image](https://github.com/holsoni/reingeneering-lab2/assets/70883268/4874c6a5-4e35-445d-98ad-7abddb448efe)

   як бачимо різниця використання Heap Memory досить велика (поле USED)
